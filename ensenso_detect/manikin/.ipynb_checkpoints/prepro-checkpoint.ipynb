{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#a good example is provided here\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "from PIL import Image\n",
    "from os import listdir\n",
    "import json\n",
    "import numpy as np\n",
    "from random import shuffle\n",
    "\n",
    "# globals \n",
    "disp = False\n",
    "\n",
    "def loadImages(path):\n",
    "    # return array of images\n",
    "\n",
    "    imagesList = listdir(path)\n",
    "    loadedImages = []\n",
    "    for image in imagesList:\n",
    "        img = Image.open(path + image)\n",
    "        loadedImages.append(img)\n",
    "\n",
    "    return loadedImages\n",
    "\n",
    "#define prepro transform\n",
    "'''\n",
    "from:\n",
    "https://github.com/pytorch/examples/blob/409a7262dcfa7906a92aeac25ee7d413baa88b67/imagenet/main.py#L108-L113\n",
    "https://github.com/pytorch/examples/blob/409a7262dcfa7906a92aeac25ee7d413baa88b67/imagenet/main.py#L94-L95\n",
    "'''\n",
    "normalize = transforms.Normalize(\n",
    "   mean=[0.485, 0.456, 0.406],\n",
    "   std=[0.229, 0.224, 0.225]\n",
    ")\n",
    "preprocess = transforms.Compose([\n",
    "   transforms.Scale(256),\n",
    "   transforms.CenterCrop(224),\n",
    "   transforms.ToTensor(),\n",
    "#    normalize\n",
    "])\n",
    "\n",
    "#provide path to true and fake images\n",
    "true_path= \"raw/true/\"\n",
    "fake_path= \"raw/negative/\"\n",
    "\n",
    "# load images in the dir\n",
    "true_images = loadImages(true_path)\n",
    "fake_images = loadImages(fake_path)\n",
    "\n",
    "#be sure the images are rightly loaded\n",
    "if disp:\n",
    "    true_images[0].show()\n",
    "    fake_images[0].show()\n",
    "\n",
    "# Now preprocess the image\n",
    "\n",
    "#define tensors to hold the images in memory\n",
    "fake_tensors, true_tensors = [], []\n",
    "\n",
    "for imgs in true_images:\n",
    "    true_temp = preprocess(imgs)\n",
    "    true_temp = Variable(true_temp)\n",
    "    true_tensors.append(true_temp)\n",
    "\n",
    "for imgs in fake_images:\n",
    "    fake_temp = preprocess(imgs)\n",
    "    fake_temp = fake_temp.view(-1, 3,)\n",
    "    fake_temp = Variable(fake_temp)\n",
    "    fake_tensors.append(fake_temp)\n",
    "\n",
    "#shuffle the images\n",
    "shuffle(true_images)\n",
    "shuffle(fake_images)\n",
    "# fakenreal_tensors = fake_tensors + true_tensors\n",
    "\n",
    "# print(fakenreal_tensors[240])\n",
    " \n",
    "# #be sure the images are properly loaded in memory\n",
    "# print(\"Total # of true tensors: \", len(true_tensors), \"true images size: \", true_tensors[0].size())\n",
    "# print(\"Total # of fake tensors: \", len(fake_tensors), \"fake images size: \", fake_tensors[0].size())\n",
    "# print(\"True Var Size {}\".format(true_tensor_variables[0].size()))\n",
    "# print(\"Fake Var Size {}\".format(fake_tensor_variables[0].size()))\n",
    "\n",
    "#load labels file\n",
    "labels_file = open('labels.json').read()\n",
    "labels = np.fromiter(json.loads(labels_file),int)\n",
    "\n",
    "#take labels to tensor\n",
    "labels = torch.from_numpy(labels)\n",
    "labels_var = Variable(labels)\n",
    "# labels = labels_var.unsqueeze(1).expand(99, 2,1)\n",
    "\n",
    "if disp:\n",
    "    print(labels)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_X and train_Y sizes: torch.Size([80, 3, 224, 224]) | torch.Size([80, 1])\n",
      "test_X and test_Y sizes: torch.Size([20, 3, 224, 224]) | torch.Size([20, 1])\n"
     ]
    }
   ],
   "source": [
    "#Now separate true and fake to training and testing sets\n",
    "#we'll do 80:20 ratio\n",
    "X_tr = int(0.8*len(true_tensors))\n",
    "X_te = len(true_tensors) - X_tr\n",
    "\n",
    "# allocate tensors memory\n",
    "train_X = torch.LongTensor(X_tr, true_tensors[0].size(0), true_tensors[0].size(1),\n",
    "                          true_tensors[0].size(2))\n",
    "train_Y = torch.LongTensor(train_X.size(0), 1)\n",
    "\n",
    "#create testing set\n",
    "test_X = torch.LongTensor(X_te, true_tensors[0].size(0), true_tensors[0].size(1),\n",
    "                          true_tensors[0].size(2))\n",
    "test_Y = torch.LongTensor(test_X.size(0), 2)\n",
    "\n",
    "#Now copy tensors over \n",
    "train_X = torch.stack(true_tensors[:X_tr], 0)\n",
    "train_Y = Variable(torch.stack(labels.unsqueeze(0).expand(int(X_tr/2), 2), 0).view(-1, X_tr).t())\n",
    "\n",
    "#testing set\n",
    "test_X = torch.stack(true_tensors[(X_tr):], 0)\n",
    "test_Y = Variable(torch.stack(labels.unsqueeze(0).expand(int(X_te/2), 2), 0).view(-1, X_te).t())\n",
    "                                      \n",
    "#check size of slices\n",
    "print('train_X and train_Y sizes: {} | {}'.format(train_X.size(), train_Y.size()))\n",
    "print('test_X and test_Y sizes: {} | {}'.format(test_X.size(), test_Y.size()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now build the CNN module\n",
    "# CNN Model (2 conv layer)\n",
    "import torch.nn.functional as F\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        '''\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, \n",
    "            padding=0, dilation=1, groups=1, bias=True)\n",
    "\n",
    "            in_channels (int) – Number of channels in the input image\n",
    "            out_channels (int) – Number of channels produced by the convolution\n",
    "            kernel_size (int or tuple) – Size of the convolving kernel\n",
    "            stride (int or tuple, optional) – Stride of the convolution\n",
    "            padding (int or tuple, optional) – Zero-padding added to both sides of the input\n",
    "            dilation (int or tuple, optional) – Spacing between kernel elements\n",
    "            groups (int, optional) – Number of blocked connections from input channels to output channels\n",
    "            bias (bool, optional) – If True, adds a learnable bias to the output\n",
    "        '''\n",
    "        self.conv1 = nn.Conv2d(3, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['face_neg', 'face_pos', 'face-positive.tar.gz']\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'fake_pos' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-eda76e20d00d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mface_neg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpwd\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/raw/face_neg'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfake_pos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mface_neg_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mface_neg\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'neg_1'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mface_neg_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mface_neg\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'neg_2'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'fake_pos' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from os import listdir\n",
    "from PIL import Image\n",
    "\n",
    "pwd = os.getcwd()\n",
    "print(listdir(pwd + '/raw'))\n",
    "face_pos = pwd + '/raw/face_pos'\n",
    "face_neg = pwd + '/raw/face_neg'\n",
    "\n",
    "print(listdir(fake_pos))\n",
    "face_neg_1 = 'manikin/raw/face_neg' + '/' + 'neg_1'\n",
    "face_neg_2 = 'manikin/raw/face_neg' + '/' + 'neg_2'\n",
    "face_neg_3 = 'manikin/raw/face_neg' + '/' + 'neg_3'\n",
    "face_neg_4 = 'manikin/raw/face_neg' + '/' + 'neg_4'\n",
    "\n",
    "negative_list = []\n",
    "\n",
    "for dirs in [face_neg_1, face_neg_2, face_neg_3, face_neg_4]:\n",
    "    for img_path in listdir(dirs):\n",
    "        base, ext = os.path.splitext(img_path)\n",
    "        if ext == '.jpg':\n",
    "            img = Image.open(img_path)\n",
    "            negative_list.append(img)\n",
    "            print('appending {} to {} list'.format(img_path, 'negative'))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'module' object has no attribute 'get'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-ff2dafde8617>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhelp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'module' object has no attribute 'get'"
     ]
    }
   ],
   "source": [
    "help(os.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
