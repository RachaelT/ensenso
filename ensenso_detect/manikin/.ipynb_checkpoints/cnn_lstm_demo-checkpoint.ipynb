{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method OrderedDict.keys of OrderedDict([('lstm1.weight_ih_l0', \n",
      "-8.3120e-03  1.2229e-02  3.6626e-03  ...   1.0745e-02 -8.7211e-03 -1.5729e-03\n",
      " 1.1770e-02  8.0955e-03 -8.6257e-03  ...   1.5129e-02  7.3832e-03  1.3534e-02\n",
      "-1.0262e-02  7.2235e-03 -9.3444e-03  ...  -1.0786e-02 -5.3581e-03 -1.0654e-02\n",
      "                ...                   ⋱                   ...                \n",
      " 8.9649e-03  5.1380e-03  3.0401e-03  ...   7.2392e-03 -1.4466e-02  1.1002e-02\n",
      "-1.3891e-02  1.4393e-02 -9.4514e-03  ...   1.5096e-02  8.0308e-03  6.3942e-04\n",
      "-4.8369e-03  5.4814e-03  2.7434e-03  ...  -1.0601e-03  1.3128e-02 -1.0100e-02\n",
      "[torch.DoubleTensor of size 16384x4096]\n",
      "), ('lstm1.weight_hh_l0', \n",
      " 9.0681e-03  8.3512e-03  1.3259e-02  ...   7.6777e-03 -1.2583e-02  6.7293e-03\n",
      " 1.0038e-02 -5.6466e-03 -3.2015e-03  ...   5.7920e-03 -9.6507e-03 -9.7868e-03\n",
      "-1.2772e-02 -1.2567e-02 -3.3892e-03  ...  -6.2676e-03  1.0279e-02  4.5665e-03\n",
      "                ...                   ⋱                   ...                \n",
      "-7.3647e-03  8.1241e-03 -8.5932e-03  ...  -1.0691e-02  1.0714e-02 -6.9571e-03\n",
      "-1.0909e-02  4.1533e-03 -7.1867e-03  ...  -1.3236e-02 -6.4171e-03  1.5319e-03\n",
      " 7.5106e-03  6.4188e-03  3.8898e-03  ...   7.8048e-03  1.1788e-02 -2.3650e-04\n",
      "[torch.DoubleTensor of size 16384x4096]\n",
      "), ('lstm2.weight_ih_l0', \n",
      "-2.5146e-03  2.1260e-02 -2.9394e-02  ...  -3.1298e-03  2.5282e-02 -7.5236e-03\n",
      " 2.5535e-03  1.6201e-02 -9.2805e-03  ...   3.4387e-03 -2.7266e-02  4.8494e-03\n",
      " 1.6374e-03  2.8726e-02 -2.4216e-02  ...   1.6411e-02  1.8289e-02  1.8194e-02\n",
      "                ...                   ⋱                   ...                \n",
      "-1.9026e-03  6.2290e-03 -2.7997e-02  ...   1.4752e-02 -8.5351e-03 -1.3339e-02\n",
      " 2.0920e-02  5.8202e-03  6.1783e-03  ...   1.2403e-02  3.0006e-02 -7.8444e-03\n",
      "-2.6174e-03 -5.8558e-04  2.1114e-02  ...  -1.4445e-02  2.4024e-02 -2.5441e-02\n",
      "[torch.DoubleTensor of size 4096x4096]\n",
      "), ('lstm2.weight_hh_l0', \n",
      " 2.6991e-02 -2.5666e-02 -1.3833e-02  ...   1.0607e-02  2.4720e-02 -1.0858e-02\n",
      "-6.1580e-03  2.6632e-02 -1.4935e-02  ...  -6.6396e-03  1.1162e-04 -1.6596e-03\n",
      " 2.4643e-02 -1.5914e-02  3.0812e-02  ...  -7.7535e-03  2.1454e-02  9.7609e-03\n",
      "                ...                   ⋱                   ...                \n",
      "-3.0703e-02  1.7707e-02  9.1310e-04  ...  -1.0758e-02  3.0963e-02 -2.0343e-02\n",
      "-2.0886e-02 -6.7411e-03 -2.7328e-02  ...  -1.3424e-02 -9.6237e-05 -1.3324e-02\n",
      "-2.6809e-02 -5.1521e-03 -2.2948e-02  ...  -8.6834e-03  9.3513e-03 -6.9752e-03\n",
      "[torch.DoubleTensor of size 4096x1024]\n",
      "), ('lstm3.weight_ih_l0', \n",
      "-4.0340e-03  4.1465e-04 -1.5683e-02  ...  -1.1026e-02 -1.0947e-02  1.6227e-02\n",
      " 1.7910e-02  5.4558e-03 -1.9902e-03  ...  -1.4846e-02 -1.8834e-02 -5.4759e-03\n",
      "-1.7049e-02 -1.3265e-02  9.0059e-04  ...   1.3888e-02 -1.3095e-02  6.8316e-03\n",
      "                ...                   ⋱                   ...                \n",
      " 1.6905e-02  1.0512e-02 -1.9430e-02  ...   1.2730e-02 -2.1872e-02  1.3590e-03\n",
      "-1.8652e-02  5.4559e-03 -4.5743e-03  ...   1.1046e-02 -1.1336e-02  1.3728e-02\n",
      "-9.3221e-03 -1.7549e-02  2.4111e-03  ...  -2.1925e-02  3.6455e-03  4.9774e-03\n",
      "[torch.DoubleTensor of size 8192x1024]\n",
      "), ('lstm3.weight_hh_l0', \n",
      "-2.1804e-02 -1.7305e-02 -1.2676e-02  ...   8.5824e-03 -3.9467e-03  4.3354e-03\n",
      "-3.5676e-03  8.7299e-03  1.7373e-02  ...  -1.6218e-02  6.6952e-03 -8.8986e-03\n",
      " 2.2167e-03 -3.0886e-03 -1.1764e-02  ...   3.9540e-03  1.2533e-02  4.8047e-03\n",
      "                ...                   ⋱                   ...                \n",
      " 1.1770e-02  6.7463e-03  2.8238e-03  ...  -1.1753e-02  1.2864e-02  1.4006e-02\n",
      "-1.0686e-02 -1.8376e-02  7.6939e-03  ...   1.5753e-02  7.8034e-03  5.5767e-03\n",
      "-1.1706e-02 -1.0713e-02  1.1022e-02  ...   6.5962e-03  1.8819e-02 -2.0634e-02\n",
      "[torch.DoubleTensor of size 8192x2048]\n",
      ")])>\n",
      "<bound method OrderedDict.keys of OrderedDict()>\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "from model import RecurrentModel\n",
    "reg = RecurrentModel(inputSize=4096, nHidden=[4096,1024, 64*32], \\\n",
    "                     noutputs=64*32, batchSize=1, ship2gpu=True, \\\n",
    "                     numLayers=1)\n",
    "\n",
    "# for m in reg.modules(): \n",
    "#     if(isinstance(m, nn.LSTM)): \n",
    "#         mkeys = m.state_dict().keys()\n",
    "#         mvals = m.state_dict().values()\n",
    "#         init.uniform(mvals[1], 0, 1)\n",
    "#         print(mvals[1])\n",
    "\"\"\"\n",
    "for m in reg.lstm3.modules(): \n",
    "    for t in m.state_dict().values():\n",
    "        print init.uniform(t, 0, 1)\n",
    "\"\"\"\n",
    "\n",
    "for m in reg.modules():\n",
    "    if (isinstance, nn.LSTM):\n",
    "        keys = m.state_dict().keys()\n",
    "        print(keys)\n",
    "        print lstm1.weight_ih_l0\n",
    "#weight_ih_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on LSTM in module torch.nn.modules.rnn object:\n",
      "\n",
      "class LSTM(RNNBase)\n",
      " |  Applies a multi-layer long short-term memory (LSTM) RNN to an input sequence.\n",
      " |  \n",
      " |  \n",
      " |  For each element in the input sequence, each layer computes the following\n",
      " |  function:\n",
      " |  \n",
      " |  .. math::\n",
      " |  \n",
      " |          \\begin{array}{ll}\n",
      " |          i_t = sigmoid(W_{ii} x_t + b_{ii} + W_{hi} h_{(t-1)} + b_{hi}) \\\\\n",
      " |          f_t = sigmoid(W_{if} x_t + b_{if} + W_{hf} h_{(t-1)} + b_{hf}) \\\\\n",
      " |          g_t = \\tanh(W_{ig} x_t + b_{ig} + W_{hc} h_{(t-1)} + b_{hg}) \\\\\n",
      " |          o_t = sigmoid(W_{io} x_t + b_{io} + W_{ho} h_{(t-1)} + b_{ho}) \\\\\n",
      " |          c_t = f_t * c_{(t-1)} + i_t * g_t \\\\\n",
      " |          h_t = o_t * \\tanh(c_t)\n",
      " |          \\end{array}\n",
      " |  \n",
      " |  where :math:`h_t` is the hidden state at time `t`, :math:`c_t` is the cell state at time `t`,\n",
      " |  :math:`x_t` is the hidden state of the previous layer at time `t` or :math:`input_t` for the first layer,\n",
      " |  and :math:`i_t`, :math:`f_t`, :math:`g_t`, :math:`o_t` are the input, forget,\n",
      " |  cell, and out gates, respectively.\n",
      " |  \n",
      " |  Args:\n",
      " |      input_size: The number of expected features in the input x\n",
      " |      hidden_size: The number of features in the hidden state h\n",
      " |      num_layers: Number of recurrent layers.\n",
      " |      bias: If False, then the layer does not use bias weights b_ih and b_hh. Default: True\n",
      " |      batch_first: If True, then the input and output tensors are provided as (batch, seq, feature)\n",
      " |      dropout: If non-zero, introduces a dropout layer on the outputs of each RNN layer except the last layer\n",
      " |      bidirectional: If True, becomes a bidirectional RNN. Default: False\n",
      " |  \n",
      " |  Inputs: input, (h_0, c_0)\n",
      " |      - **input** (seq_len, batch, input_size): tensor containing the features of the input sequence.\n",
      " |        The input can also be a packed variable length sequence. See :func:`torch.nn.utils.rnn.pack_padded_sequence`\n",
      " |        for details.\n",
      " |      - **h_0** (num_layers \\* num_directions, batch, hidden_size): tensor containing\n",
      " |        the initial hidden state for each element in the batch.\n",
      " |      - **c_0** (num_layers \\* num_directions, batch, hidden_size): tensor containing\n",
      " |        the initial cell state for each element in the batch.\n",
      " |  \n",
      " |  \n",
      " |  Outputs: output, (h_n, c_n)\n",
      " |      - **output** (seq_len, batch, hidden_size * num_directions): tensor containing\n",
      " |        the output features `(h_t)` from the last layer of the RNN, for each t. If a\n",
      " |        :class:`torch.nn.utils.rnn.PackedSequence` has been given as the input, the output will also be a\n",
      " |        packed sequence.\n",
      " |      - **h_n** (num_layers * num_directions, batch, hidden_size): tensor containing the hidden state for t=seq_len\n",
      " |      - **c_n** (num_layers * num_directions, batch, hidden_size): tensor containing the cell state for t=seq_len\n",
      " |  \n",
      " |  Attributes:\n",
      " |      weight_ih_l[k] : the learnable input-hidden weights of the k-th layer `(W_ii|W_if|W_ig|W_io)`, of shape\n",
      " |                       `(input_size x 4*hidden_size)`\n",
      " |      weight_hh_l[k] : the learnable hidden-hidden weights of the k-th layer `(W_hi|W_hf|W_hg|W_ho)`, of shape\n",
      " |                       `(hidden_size x 4*hidden_size)`\n",
      " |      bias_ih_l[k] : the learnable input-hidden bias of the k-th layer `(b_ii|b_if|b_ig|b_io)`, of shape\n",
      " |                       `(4*hidden_size)`\n",
      " |      bias_hh_l[k] : the learnable hidden-hidden bias of the k-th layer `(W_hi|W_hf|W_hg|b_ho)`, of shape\n",
      " |                       `(4*hidden_size)`\n",
      " |  \n",
      " |  Examples::\n",
      " |  \n",
      " |      >>> rnn = nn.LSTM(10, 20, 2)\n",
      " |      >>> input = Variable(torch.randn(5, 3, 10))\n",
      " |      >>> h0 = Variable(torch.randn(2, 3, 20))\n",
      " |      >>> c0 = Variable(torch.randn(2, 3, 20))\n",
      " |      >>> output, hn = rnn(input, (h0, c0))\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      LSTM\n",
      " |      RNNBase\n",
      " |      torch.nn.modules.module.Module\n",
      " |      __builtin__.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, *args, **kwargs)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from RNNBase:\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |  \n",
      " |  __setstate__(self, d)\n",
      " |  \n",
      " |  forward(self, input, hx=None)\n",
      " |  \n",
      " |  reset_parameters(self)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from RNNBase:\n",
      " |  \n",
      " |  all_weights\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from torch.nn.modules.module.Module:\n",
      " |  \n",
      " |  __call__(self, *input, **kwargs)\n",
      " |  \n",
      " |  __delattr__(self, name)\n",
      " |  \n",
      " |  __dir__(self)\n",
      " |  \n",
      " |  __getattr__(self, name)\n",
      " |  \n",
      " |  __setattr__(self, name, value)\n",
      " |  \n",
      " |  add_module(self, name, module)\n",
      " |      Adds a child module to the current module.\n",
      " |      \n",
      " |      The module can be accessed as an attribute using the given name.\n",
      " |  \n",
      " |  apply(self, fn)\n",
      " |  \n",
      " |  children(self)\n",
      " |      Returns an iterator over immediate children modules.\n",
      " |  \n",
      " |  cpu(self, device_id=None)\n",
      " |      Moves all model parameters and buffers to the CPU.\n",
      " |  \n",
      " |  cuda(self, device_id=None)\n",
      " |      Moves all model parameters and buffers to the GPU.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          device_id (int, optional): if specified, all parameters will be\n",
      " |              copied to that device\n",
      " |  \n",
      " |  double(self)\n",
      " |      Casts all parameters and buffers to double datatype.\n",
      " |  \n",
      " |  eval(self)\n",
      " |      Sets the module in evaluation mode.\n",
      " |      \n",
      " |      This has any effect only on modules such as Dropout or BatchNorm.\n",
      " |  \n",
      " |  float(self)\n",
      " |      Casts all parameters and buffers to float datatype.\n",
      " |  \n",
      " |  half(self)\n",
      " |      Casts all parameters and buffers to half datatype.\n",
      " |  \n",
      " |  load_state_dict(self, state_dict)\n",
      " |      Copies parameters and buffers from :attr:`state_dict` into\n",
      " |      this module and its descendants. The keys of :attr:`state_dict` must\n",
      " |      exactly match the keys returned by this module's :func:`state_dict()`\n",
      " |      function.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          state_dict (dict): A dict containing parameters and\n",
      " |              persistent buffers.\n",
      " |  \n",
      " |  modules(self)\n",
      " |      Returns an iterator over all modules in the network.\n",
      " |      \n",
      " |      Note:\n",
      " |          Duplicate modules are returned only once. In the following\n",
      " |          example, ``l`` will be returned only once.\n",
      " |      \n",
      " |          >>> l = nn.Linear(2, 2)\n",
      " |          >>> net = nn.Sequential(l, l)\n",
      " |          >>> for idx, m in enumerate(net.modules()):\n",
      " |          >>>     print(idx, '->', m)\n",
      " |          0 -> Sequential (\n",
      " |            (0): Linear (2 -> 2)\n",
      " |            (1): Linear (2 -> 2)\n",
      " |          )\n",
      " |          1 -> Linear (2 -> 2)\n",
      " |  \n",
      " |  named_children(self)\n",
      " |      Returns an iterator over immediate children modules, yielding both\n",
      " |      the name of the module as well as the module itself.\n",
      " |      \n",
      " |      Example:\n",
      " |          >>> for name, module in model.named_children():\n",
      " |          >>>     if name in ['conv4', 'conv5']:\n",
      " |          >>>         print(module)\n",
      " |  \n",
      " |  named_modules(self, memo=None, prefix='')\n",
      " |      Returns an iterator over all modules in the network, yielding\n",
      " |      both the name of the module as well as the module itself.\n",
      " |      \n",
      " |      Note:\n",
      " |          Duplicate modules are returned only once. In the following\n",
      " |          example, ``l`` will be returned only once.\n",
      " |      \n",
      " |          >>> l = nn.Linear(2, 2)\n",
      " |          >>> net = nn.Sequential(l, l)\n",
      " |          >>> for idx, m in enumerate(net.named_modules()):\n",
      " |          >>>     print(idx, '->', m)\n",
      " |          0 -> ('', Sequential (\n",
      " |            (0): Linear (2 -> 2)\n",
      " |            (1): Linear (2 -> 2)\n",
      " |          ))\n",
      " |          1 -> ('0', Linear (2 -> 2))\n",
      " |  \n",
      " |  named_parameters(self, memo=None, prefix='')\n",
      " |      Returns an iterator over module parameters, yielding both the\n",
      " |      name of the parameter as well as the parameter itself\n",
      " |      \n",
      " |      Example:\n",
      " |          >>> for name, param in self.named_parameters():\n",
      " |          >>>    if name in ['bias']:\n",
      " |          >>>        print(param.size())\n",
      " |  \n",
      " |  parameters(self, memo=None)\n",
      " |      Returns an iterator over module parameters.\n",
      " |      \n",
      " |      This is typically passed to an optimizer.\n",
      " |      \n",
      " |      Example:\n",
      " |          >>> for param in model.parameters():\n",
      " |          >>>     print(type(param.data), param.size())\n",
      " |          <class 'torch.FloatTensor'> (20L,)\n",
      " |          <class 'torch.FloatTensor'> (20L, 1L, 5L, 5L)\n",
      " |  \n",
      " |  register_backward_hook(self, hook)\n",
      " |      Registers a backward hook on the module.\n",
      " |      \n",
      " |      The hook will be called every time the gradients with respect to module\n",
      " |      inputs are computed. The hook should have the following signature::\n",
      " |      \n",
      " |          hook(module, grad_input, grad_output) -> Tensor or None\n",
      " |      \n",
      " |      The :attr:`grad_input` and :attr:`grad_output` may be tuples if the\n",
      " |      module has multiple inputs or outputs. The hook should not modify its\n",
      " |      arguments, but it can optionally return a new gradient with respect to\n",
      " |      input that will be used in place of :attr:`grad_input` in subsequent\n",
      " |      computations.\n",
      " |      \n",
      " |      This function returns a handle with a method ``handle.remove()``\n",
      " |      that removes the hook from the module.\n",
      " |  \n",
      " |  register_buffer(self, name, tensor)\n",
      " |      Adds a persistent buffer to the module.\n",
      " |      \n",
      " |      This is typically used to register a buffer that should not to be\n",
      " |      considered a model parameter. For example, BatchNorm's ``running_mean``\n",
      " |      is not a parameter, but is part of the persistent state.\n",
      " |      \n",
      " |      Buffers can be accessed as attributes using given names.\n",
      " |      \n",
      " |      Example:\n",
      " |          >>> self.register_buffer('running_mean', torch.zeros(num_features))\n",
      " |  \n",
      " |  register_forward_hook(self, hook)\n",
      " |      Registers a forward hook on the module.\n",
      " |      \n",
      " |      The hook will be called every time :func:`forward` computes an output.\n",
      " |      It should have the following signature::\n",
      " |      \n",
      " |          hook(module, input, output) -> None\n",
      " |      \n",
      " |      The hook should not modify the input or output.\n",
      " |      This function returns a handle with a method ``handle.remove()``\n",
      " |      that removes the hook from the module.\n",
      " |  \n",
      " |  register_parameter(self, name, param)\n",
      " |      Adds a parameter to the module.\n",
      " |      \n",
      " |      The parameter can be accessed as an attribute using given name.\n",
      " |  \n",
      " |  share_memory(self)\n",
      " |  \n",
      " |  state_dict(self, destination=None, prefix='')\n",
      " |      Returns a dictionary containing a whole state of the module.\n",
      " |      \n",
      " |      Both parameters and persistent buffers (e.g. running averages) are\n",
      " |      included. Keys are corresponding parameter and buffer names.\n",
      " |      \n",
      " |      Example:\n",
      " |          >>> module.state_dict().keys()\n",
      " |          ['bias', 'weight']\n",
      " |  \n",
      " |  train(self, mode=True)\n",
      " |      Sets the module in training mode.\n",
      " |      \n",
      " |      This has any effect only on modules such as Dropout or BatchNorm.\n",
      " |  \n",
      " |  type(self, dst_type)\n",
      " |  \n",
      " |  zero_grad(self)\n",
      " |      Sets gradients of all model parameters to zero.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from torch.nn.modules.module.Module:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from torch.nn.modules.module.Module:\n",
      " |  \n",
      " |  dump_patches = False\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['weight_ih_l0', 'weight_hh_l0']\n"
     ]
    }
   ],
   "source": [
    "mkeys = m.state_dict().keys()\n",
    "print mkeys"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
